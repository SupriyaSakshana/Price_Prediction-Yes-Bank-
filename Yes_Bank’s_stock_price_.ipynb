{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "HAih1iBOpsJ2",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupriyaSakshana/Price_Prediction-Yes-Bank-/blob/main/Yes_Bank%E2%80%99s_stock_price_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "O1Fg6YqxbReP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Name - Yes Bank Stock Price Forecast Using Machine Learning"
      ],
      "metadata": {
        "id": "fl5hWDTvgKDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The objective of this project is to analyze the impact of a fraud case involving Rana Kapoor on the stock prices of Yes Bank, a prominent bank in the Indian financial domain. The dataset used in this project consisted of monthly stock prices of Yes Bank since its inception, including closing, starting, highest, and lowest stock prices.\n",
        "\n",
        "The analysis aimed to uncover any patterns or changes in stock prices related to the fraud case involving Rana Kapoor.\n",
        "\n",
        "Overall, the project aimed to contribute to a better understanding of the relationship between the fraud case and Yes Bank's stock prices, and to explore the potential of predictive models in the financial domain. The findings and insights gained from this project can be utilized by investors, analysts, and decision-makers to make informed investment or business decisions related to Yes Bank's stock."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n",
        "\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month. The main objective is to predict the stock‚Äôs closing price of the month.\n",
        "\n",
        "A Stock or share (also known as a company‚Äôs 'equity') is a financial instrument that represents ownership in a company. Units of stock are called \"shares.\" Stocks are bought and sold predominantly on stock exchanges, though there can be private sales as well, and are the foundation of many individual investors' portfolios.\n",
        "\n",
        "Business Objective.\n",
        "\n",
        "The ultimate business objective is to leverage the regression model to provide accurate predictions of the closing price of Yes Bank stock, enabling stakeholders to make well-informed investment decisions, manage risks effectively, optimize portfolios, Early warning systems to alert any fraud cases like Rana Kapoor and align investment strategies with financial goals.\n",
        "\n",
        "Steps involved are:-\n",
        "\n",
        "1.DATA PREPROCESSING\n",
        "\n",
        "2.DATA CLEANING\n",
        "\n",
        "3.DATA DUPLICATION\n",
        "\n",
        "4.HANDLING OUTLIERS\n",
        "\n",
        "5.FEATURE TRANSFORMATION\n",
        "\n",
        "6.EXPLORATORY DATA ANALYSIS\n",
        "\n",
        "7.ENCODING OF CATEGORICAL COLUMNS\n",
        "\n",
        "8.ALGORITHMS:-\n",
        "\n",
        "a.Linear Regression b.Ridge Regression c.Random Forest Regressor d.XGBoost Regressor"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np                                                           #numerical computations\n",
        "import pandas as pd                                                          #to load the data\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns                                                        # for visualization\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import (MinMaxScaler,StandardScaler)              #scaling the data\n",
        "from sklearn.model_selection import train_test_split                        # split train and test data\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.linear_model import (Lasso, Ridge,ElasticNet, LassoCV, RidgeCV, ElasticNetCV)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/data_YesBank_StockPrices.csv')"
      ],
      "metadata": {
        "id": "iA7j-LHXjD9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(dataset[dataset.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set figure size (change as you like)\n",
        "plt.figure(figsize=(3, 2))\n",
        "\n",
        "# Create missing values bar plot\n",
        "msno.bar(\n",
        "    dataset,\n",
        "    color=\"#0A1F44\",   # Metallic Dark Blue color\n",
        ")\n",
        "\n",
        "# Title styling\n",
        "plt.title(\n",
        "    \"Missing Data Bar Plot\",\n",
        "    fontsize=14,\n",
        "    fontweight=\"bold\",\n",
        "    color=\"#0A1F44\"\n",
        ")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Basic understanding of the dataset**\n",
        "\n",
        "My dataset contains historical stock price data of Yes Bank.\n",
        "\n",
        "It has 185 records\n",
        "\n",
        "Each record represents one month\n",
        "\n",
        "The data spans multiple years (from Jul-2005 onwards)\n",
        "\n",
        "It is a time-series financial dataset\n",
        "\n",
        "\n",
        "üëâ All price columns are numerical (float)\n",
        "üëâ Date is a time-based categorical feature\n",
        "\n",
        "**Nature of the data**\n",
        "\n",
        "No missing values (clean dataset)\n",
        "\n",
        "Data is continuous and numerical\n",
        "\n",
        "Prices show high volatility, which is common in stock market data\n",
        "\n",
        "The dataset is chronologically ordered, which is important for time-series analysis\n",
        "\n",
        "**Statistical insights**\n",
        "\n",
        "From exploratory analysis:\n",
        "\n",
        "Average closing price ‚âà 105\n",
        "\n",
        "Stock prices range from ~10 to ~400\n",
        "\n",
        "High standard deviation indicates large fluctuations\n",
        "\n",
        "Clear growth and decline phases are visible over time\n",
        "\n",
        "This shows:\n",
        "\n",
        "The stock experienced significant ups and downs, making it suitable for trend analysis and forecasting.\n",
        "\n",
        "** Type of problem this dataset supports**\n",
        "\n",
        "This dataset can be used for:\n",
        "\n",
        "**Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Trend analysis\n",
        "\n",
        "Stock price prediction\n",
        "\n",
        "Time-series forecasting\n",
        "\n",
        "Regression problems\n",
        "\n",
        "**Limitations of the dataset**\n",
        "\n",
        "No volume data\n",
        "\n",
        "No external factors (news, market index, interest rates)\n",
        "\n",
        "Monthly data (not daily), so short-term patterns are not visible"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features (columns) in the dataset**\n",
        "\n",
        "The dataset has 5 columns:\n",
        "\n",
        "Column\tDescription\n",
        "\n",
        "Date\tMonth and year of the stock price\n",
        "\n",
        "Open\tOpening price of the stock\n",
        "\n",
        "High\tHighest price during the month\n",
        "\n",
        "Low\tLowest price during the month\n",
        "\n",
        "Close\tClosing price of the stock"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for col in dataset.columns:\n",
        "    print(f\"\\n{col} Unique Values:\")\n",
        "    print(dataset[col].unique())\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or"
      ],
      "metadata": {
        "id": "cPkwUfOYp0dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.nunique()\n"
      ],
      "metadata": {
        "id": "aeQI9Mhsp1mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***\n",
        "\n",
        "Cleaning, organizing, and preparing raw data so that it can be used for analysis or machine learning."
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "dataset_copy= dataset.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy.head()"
      ],
      "metadata": {
        "id": "v5sZ1B_iO-87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy.isnull().sum()"
      ],
      "metadata": {
        "id": "fDQNhV2uPRfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_copy[dataset_copy.duplicated()])"
      ],
      "metadata": {
        "id": "ey9wbcoOPZk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting datatype of Date from string to Datetime\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "dataset_copy['Date']=pd.to_datetime(dataset_copy['Date'].apply(lambda x: datetime.strptime(x,'%b-%y')))"
      ],
      "metadata": {
        "id": "9BjEfe8YPjyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy.head()"
      ],
      "metadata": {
        "id": "GZEGfhpLPmPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy.info()"
      ],
      "metadata": {
        "id": "fMMPclWyP9Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy.describe()"
      ],
      "metadata": {
        "id": "iP7XHYhVQJJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col=dataset_copy.columns.to_list()\n",
        "numerical_cols=col[1:]"
      ],
      "metadata": {
        "id": "IdQzJLoxQTEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols"
      ],
      "metadata": {
        "id": "XD444JJFQYTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in numerical_cols:\n",
        "  plt.figure(figsize=(6,6))\n",
        "  sns.boxplot(dataset_copy[column],orient='h')\n",
        "  plt.xlabel(column, fontsize=10)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "np2Epin8Qm8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the Date as index.\n",
        "dataset_copy.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "id": "PzlExeB8Q1VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy.head()"
      ],
      "metadata": {
        "id": "iwaRxWBgQ6jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seperating the data\n",
        "independent_variables = dataset_copy.columns.tolist()[:-1]\n",
        "dependent_variable = ['Close']\n",
        "\n",
        "print(independent_variables)\n",
        "print(dependent_variable)"
      ],
      "metadata": {
        "id": "sBXVJc0IRA4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What all manipulations have you done and insights you found?\n",
        "I have changed datatype of Date variable to datetime. Remaining all variables are numerical. numerical variables are Open,High, Low and Close(dependent variable). There are no null values and duplicate values. Data is clean and ready for vizualization. Mean is higher in numerical columns which indicate possibility of skewness in the data."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(8,7))\n",
        "dataset_copy['Close'].plot(color = 'b')\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Closing Price yearly')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how closing price in each year."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "form seeing the plot, it is evident that after the fraud case in 2018, the closing price took a huge hit and dropped significantly"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "yes, because of the 2018 fraud the yes bank stock has suffered immensily from 2018. That should not happen in the future."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependent variable 'Close'\n",
        "#checking the distribution of the dependent variable\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.distplot(dataset_copy['Close'],color=\"b\")\n",
        "plt.title('Distibution of Dependent variable')\n",
        "plt.xlabel('closing price')\n",
        "\n",
        "plt.axvline(dataset_copy['Close'].mean(),color='yellow')\n",
        "plt.axvline(dataset_copy['Close'].median(),color='red',linestyle='dashed')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the distribution of the Dependent variable. distplot gives more accurate result."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the data is somewhat positively skewed(right skew)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, by observing the chart we now know that the closing price will always change over a period of time and that too because of the fraud in 2018, data is skewed positively. transformation need to be applied which will result in better prediction of closing price."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#applying log transformation\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.distplot(np.log10(dataset_copy['Close']),color=\"b\")\n",
        "plt.title('Distibution of Transformed Dependent variable')\n",
        "plt.xlabel('closing price')\n",
        "\n",
        "\n",
        "plt.axvline(np.log10(dataset_copy['Close']).mean(),color='yellow')\n",
        "plt.axvline(np.log10(dataset_copy['Close']).median(),color='red',linestyle='dashed')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the distribution of y variable"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the log transformation it nearly looks like normal distribution. mean and median are almost same"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the data is normally distributed, it becomes easy to develop a good model"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4   Without Transformation (Original Data)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distributions of Independent features.\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "for i, col in enumerate(independent_variables):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    sns.distplot(dataset_copy[col], color='b')\n",
        "    plt.xlabel(col, fontsize=10)\n",
        "\n",
        "    # Plotting the mean and the median.\n",
        "    plt.axvline(dataset_copy[col].mean(), color='yellow')\n",
        "    plt.axvline(dataset_copy[col].median(), color='red', linestyle='dashed')\n",
        "\n",
        "plt.suptitle('Distibution of independent variables')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the distributions across all independent variables."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that all the independent variables are right skewed and transformation is required"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "By observing above chart I came to know that transformation need to be applied on all independent variables which will essentially require for a good model"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5  With Log Transformation"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of transformed independent variables\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "for i, col in enumerate(independent_variables):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    sns.distplot(np.log10(dataset_copy[col]), color='b')\n",
        "    plt.xlabel(col, fontsize=10)\n",
        "\n",
        "    # Plotting the mean and the median.\n",
        "    plt.axvline(np.log10(dataset_copy[col]).mean(), color='yellow')\n",
        "    plt.axvline(np.log10(dataset_copy[col]).median(), color='red', linestyle='dashed')\n",
        "\n",
        "plt.suptitle('Distibution of Transformed independent variables')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the distributions across all independent variables after transformation applied."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the log Transformation, the data of all indepedent variables closely follow normal distribution"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build best model if the data is normally distributed."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mplfinance\n",
        "\n",
        "import mplfinance as mpf\n",
        "\n",
        "df_candle = dataset_copy[['Open', 'High', 'Low', 'Close']]\n",
        "\n",
        "mpf.plot(df_candle, type='candle', style='yahoo', title='Candlestick Chart')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Candle sticks are most widely used finance world. with this chart I wnat to see how variables are each month."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart, there is no much deviation from the price shown by open, high, low."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from this data can help create a positive business impact.\n",
        "\n",
        "üìå Reasons:\n",
        "\n",
        "1Ô∏è‚É£ Identification of Growth Phase (2012‚Äì2018)\n",
        "From the graph, the stock shows strong upward growth between 2012 and 2018.\n",
        "\n",
        "‚û° Business Impact:\n",
        "\n",
        "Good time for investment\n",
        "\n",
        "Opportunity for portfolio expansion\n",
        "\n",
        "High investor confidence\n",
        "\n",
        "Company valuation increases\n",
        "\n",
        "This insight helps investors and companies:\n",
        "\n",
        "Decide the right time to invest or expand.\n",
        "\n",
        "2Ô∏è‚É£ Detection of Market Peaks (2017‚Äì2018)\n",
        "The price reaches its maximum (around 350‚Äì400) in 2017‚Äì2018.\n",
        "\n",
        "‚û° Business Impact:\n",
        "\n",
        "Helps in profit booking\n",
        "\n",
        "Risk management\n",
        "\n",
        "Avoids losses\n",
        "\n",
        "This insight supports:\n",
        "\n",
        "Better exit strategies.\n",
        "\n",
        "3Ô∏è‚É£ Early Warning of Risk (Post-2018 Crash)\n",
        "After 2018, there is a sharp decline.\n",
        "\n",
        "‚û° Business Impact:\n",
        "\n",
        "Helps in loss prevention\n",
        "\n",
        "Encourages diversification\n",
        "\n",
        "Improves decision-making\n",
        "\n",
        "This insight allows:\n",
        "\n",
        "Investors to reduce exposure before major losses.\n",
        "\n",
        "‚ùå Are there any insights that lead to negative growth?\n",
        "\n",
        "Yes, the graph clearly shows a phase of negative growth.\n",
        "\n",
        "üìâ Major Negative Phase (2018‚Äì2020)\n",
        "\n",
        "From 2018 onwards, the price falls:\n",
        "\n",
        "üìâ From ~380 ‚Üí ~20\n",
        "\n",
        "This is a massive decline.\n",
        "\n",
        "üìå Specific Reasons (Possible Justification)\n",
        "\n",
        "Based on market behavior, this decline may be due to:\n",
        "\n",
        "1Ô∏è‚É£ Financial instability\n",
        "2Ô∏è‚É£ Governance issues\n",
        "3Ô∏è‚É£ Loss of investor trust\n",
        "4Ô∏è‚É£ Regulatory problems\n",
        "5Ô∏è‚É£ Poor performance\n",
        "\n",
        "‚û° In Yes Bank‚Äôs case (real-world context):\n",
        "It was mainly due to bad loans (NPAs) and management crisis."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_price = np.log(dataset_copy[['Open', 'High', 'Low']])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "df_price.boxplot()\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Box Plots for Price Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see outliers exists or not in the independent variables."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After transformation applied, outliers appeared to be diminished."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Outliers are not present in the data, this helps us building a good model, however given the dataset that small, it is also not advisable to completely remove outliers."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plotting the independent variables against dependent variable and checking the correlation between them.\n",
        "for col in independent_variables:\n",
        "\n",
        "  fig = plt.figure(figsize=(6, 3))\n",
        "  ax = fig.gca()\n",
        "  feature = dataset_copy[col]\n",
        "  label = dataset_copy['Close']\n",
        "  correlation = feature.corr(label)        # calculating the correlation between dependent variable and independent features.\n",
        "  plt.scatter(x=feature, y=label)          # plotting dependent variables against independent features.\n",
        "\n",
        "\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Close')\n",
        "  ax.set_title('Close vs ' + col + '- correlation: ' + str(round((correlation),6)))\n",
        "\n",
        "  z = np.polyfit(dataset_copy[col], dataset_copy['Close'], 1)\n",
        "  y_ = np.poly1d(z)(dataset_copy[col])\n",
        "\n",
        "  plt.plot(dataset_copy[col], y_, \"r--\", lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to plot correlation between independent variable and dependent variable."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that all independent variables are highly correlated and linear in fashion to dependent variable."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When independent variables are highly correlated to y variable, it means that linear regression is going to be good fit model and also can produce good accuracy."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Extract the date and closing price columns from the dataset\n",
        "dates = dataset_copy.index\n",
        "closing_prices = dataset_copy['Close']\n",
        "\n",
        "# Create a line plot\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(dates, closing_prices)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Trend of Closing Prices over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line chart was selected because it clearly shows the trend of closing prices over time. It helps in understanding how the stock price has changed across different years and allows easy identification of growth, stability, and decline phases.\n",
        "\n",
        "Line charts are especially useful in financial analysis because they:\n",
        "\n",
        "Show long-term performance\n",
        "\n",
        "Highlight market trends\n",
        "\n",
        "Help compare different time periods\n",
        "\n",
        "Support forecasting and decision-making\n",
        "\n",
        "Therefore, this chart is ideal for analyzing the overall behavior of Yes Bank‚Äôs stock price."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, the following key insights are observed:\n",
        "\n",
        "üìà Growth Phase (2006‚Äì2018)\n",
        "\n",
        "The stock price shows a steady and strong increase from 2006 to 2018.\n",
        "\n",
        "Major growth is visible after 2013, indicating improved business performance.\n",
        "\n",
        "The peak occurs around 2017‚Äì2018 (above ‚Çπ350).\n",
        "\n",
        "‚û° This suggests high investor confidence and strong financial performance during this period.\n",
        "\n",
        "‚öñÔ∏è Fluctuation Phase (2008‚Äì2012)\n",
        "\n",
        "The price shows moderate ups and downs.\n",
        "\n",
        "This indicates market uncertainty and slow growth.\n",
        "\n",
        "‚û° This reflects cautious investor behavior.\n",
        "\n",
        "üìâ Decline Phase (2018‚Äì2020)\n",
        "\n",
        "After 2018, the stock price falls sharply.\n",
        "\n",
        "The price drops from around ‚Çπ350 to below ‚Çπ50.\n",
        "\n",
        "‚û° This indicates serious financial and operational problems."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights can help create a positive business impact.\n",
        "\n",
        "üìå How?\n",
        "\n",
        "1Ô∏è‚É£ Better Investment Decisions\n",
        "Investors can identify profitable periods (2013‚Äì2018) and invest accordingly.\n",
        "\n",
        "2Ô∏è‚É£ Risk Management\n",
        "The sudden fall after 2018 acts as a warning sign for future investments.\n",
        "\n",
        "3Ô∏è‚É£ Strategic Planning\n",
        "Companies can analyze what factors led to growth and try to repeat those strategies.\n",
        "\n",
        "4Ô∏è‚É£ Forecasting\n",
        "Historical trends help in predicting future movements.\n",
        "\n",
        "‚û° These insights improve financial planning and reduce losses.\n",
        "\n",
        "\n",
        "Yes, the chart shows clear evidence of negative growth after 2018.\n",
        "\n",
        "üìâ Negative Growth Phase: 2018‚Äì2020\n",
        "\n",
        "During this period:\n",
        "\n",
        "Stock price declined drastically.\n",
        "\n",
        "Investor confidence reduced.\n",
        "\n",
        "Market value decreased.\n",
        "\n",
        "üìå Specific Reasons:\n",
        "\n",
        "The major reasons for this decline include:\n",
        "\n",
        "1Ô∏è‚É£ Increase in Non-Performing Assets (NPAs)\n",
        "2Ô∏è‚É£ Poor risk management\n",
        "3Ô∏è‚É£ Corporate governance issues\n",
        "4Ô∏è‚É£ Financial instability\n",
        "5Ô∏è‚É£ Regulatory intervention\n",
        "\n",
        "‚û° In Yes Bank‚Äôs case, these issues led to loss of trust and heavy selling by investors.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The chart provides valuable insights into both positive and negative phases of Yes Bank‚Äôs performance. While the growth period supports profitable investment decisions, the decline phase highlights the importance of risk assessment and financial transparency. Overall, these insights help businesses and investors make informed decisions."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Correlation Heatmap visualization code\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "correlation = dataset_copy.corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify the correlation between variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is expected that all independent variables are correlated to each other because opening price, high price, low price will not vary much in the finance sector. So, I am going to do feature generation to avoid multicollinearity."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pair Plot visualization code\n",
        "\n",
        "sns.pairplot(dataset_copy)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot was selected because it helps in visualizing the relationship between multiple variables at the same time. It shows:\n",
        "\n",
        "The distribution of each variable\n",
        "\n",
        "The relationship between every pair of variables\n",
        "\n",
        "Possible correlations\n",
        "\n",
        "Patterns and trends\n",
        "\n",
        "Outliers (unusual values)\n",
        "\n",
        "Using sns.pairplot(dataset_copy) allows us to quickly understand how different features in the dataset are related to each other, which is very useful before building a machine learning model."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the pair plot, the following insights can be observed:\n",
        "\n",
        "üìå 1. Relationship Between Variables\n",
        "\n",
        "Some variables show a strong positive correlation, meaning when one increases, the other also increases.\n",
        "\n",
        "For example, features like Open, High, Low, and Close prices usually move together.\n",
        "\n",
        "‚û° This indicates that these variables are closely related.\n",
        "\n",
        "üìå 2. Distribution of Each Feature\n",
        "\n",
        "The diagonal plots show how each variable is distributed.\n",
        "\n",
        "Some variables may be right-skewed, indicating more low values and fewer high values.\n",
        "\n",
        "Others may appear normally distributed.\n",
        "\n",
        "‚û° This helps in deciding whether data transformation is needed.\n",
        "\n",
        "üìå 3. Presence of Outliers\n",
        "\n",
        "Some plots may show extreme points far from the main cluster.\n",
        "\n",
        "These represent outliers, which can affect model performance.\n",
        "\n",
        "‚û° These may need to be treated during data preprocessing.\n",
        "\n",
        "üìå 4. Multicollinearity Detection\n",
        "\n",
        "When two variables are highly correlated, they provide similar information.\n",
        "\n",
        "This may cause multicollinearity, which affects regression models.\n",
        "\n",
        "‚û° In such cases, one variable may be removed or combined.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The pair plot helps in understanding data structure, feature relationships, and data quality. It plays an important role in feature selection, preprocessing, and improving model accuracy."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement1: There is a significant difference in the mean closing prices between the first half (2005-2017) and the second half (2018-2020) of the dataset.\n",
        "\n",
        "Statement2: There is a significant difference in the mean closing prices between months with high opening prices and months with low opening prices."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the mean closing prices between the first half and the second half of the dataset.\n",
        "\n",
        "Alternate Hypothesis (H1): There is a significant difference in the mean closing prices between the first half and the second half of the dataset"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Divide the dataset into two halves: first half and second half\n",
        "first_half = dataset_copy['Close'][dataset_copy.index.year <= 2017]\n",
        "second_half = dataset_copy['Close'][dataset_copy.index.year >= 2018]\n",
        "\n",
        "# Calculate the means and standard deviations of the two halves\n",
        "mean1 = np.mean(first_half)\n",
        "mean2 = np.mean(second_half)\n",
        "std1 = np.std(first_half)\n",
        "std2 = np.std(second_half)\n",
        "\n",
        "# Calculate the sample sizes\n",
        "n1 = len(first_half)\n",
        "n2 = len(second_half)\n",
        "\n",
        "# Calculate the standard error of the difference between means\n",
        "standard_error = np.sqrt((std1**2 / n1) + (std2**2 / n2))\n",
        "\n",
        "# Calculate the z-score\n",
        "z = (mean1 - mean2) / standard_error\n",
        "\n",
        "# Calculate the p-value (two-tailed test)\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value with the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in means.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in means.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z test, as i my data has more than 30 records."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "my dataset has more than 30 records, and i can calculate mean and sd from it."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement: There is a significant difference in the mean closing prices between months with high opening prices and months with low opening prices.\n",
        "\n",
        "Null Hypothesis (H0): The mean closing prices in months with high opening prices are equal to or lower than the mean closing prices in months with low opening prices. Alternative Hypothesis (HA): The mean closing prices in months with high opening prices are higher than the mean closing prices in months with low opening prices.."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Calculate the mean opening and closing prices for each month\n",
        "dataset_copy['Month'] = dataset_copy.index.strftime('%Y-%m')\n",
        "monthly_data = dataset_copy.groupby('Month').agg({'Open': 'mean', 'Close': 'mean'})\n",
        "\n",
        "# Define the threshold for high and low opening prices\n",
        "threshold = monthly_data['Open'].median()\n",
        "\n",
        "# Divide the data into groups based on opening prices\n",
        "high_opening_prices = monthly_data[monthly_data['Open'] > threshold]['Close']\n",
        "low_opening_prices = monthly_data[monthly_data['Open'] <= threshold]['Close']\n",
        "\n",
        "# Calculate the sample statistics\n",
        "mean_high = np.mean(high_opening_prices)\n",
        "mean_low = np.mean(low_opening_prices)\n",
        "std_high = np.std(high_opening_prices)\n",
        "std_low = np.std(low_opening_prices)\n",
        "n_high = len(high_opening_prices)\n",
        "n_low = len(low_opening_prices)\n",
        "\n",
        "# Calculate the z-statistic\n",
        "z_statistic = (mean_high - mean_low) / np.sqrt((std_high**2 / n_high) + (std_low**2 / n_low))\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 1 - stats.norm.cdf(z_statistic)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value with the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in the mean closing prices.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in the mean closing prices.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Divide the data into groups based on opening prices\n",
        "high_opening_prices = dataset_copy[dataset_copy['Open'] > threshold]['Close']\n",
        "low_opening_prices = dataset_copy[dataset_copy['Open'] <= threshold]['Close']\n",
        "\n",
        "# Perform independent t-test\n",
        "t_statistic, p_value = stats.ttest_ind(high_opening_prices, low_opening_prices)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value with the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in the mean closing prices.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in the mean closing prices.\")"
      ],
      "metadata": {
        "id": "TC4RFTCIgBXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t test and z test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As my data is sample data and not normally distrubuted. there is skewenss involved."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#There are no missing values in the dataset.\n",
        "\n",
        "dataset_copy.isnull().sum()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#There are no missing values in the dataset."
      ],
      "metadata": {
        "id": "30jJjQ3JgsjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformation has taken care of outliers, so no need to treat outliers."
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode your categorical columns\n",
        "#There are no categorical variables in this dataset."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "\n",
        "# As all the independent variables are highly correlated, I can create a new feature by takinig mean from the each record of independent variables. This will avoid MultiCollinearity and also overfitting of the model.\n",
        "\n",
        "\n",
        "dataset_copy['Mean_OHL'] = dataset_copy[['Open', 'High', 'Low']].mean(axis=1)\n",
        "dataset_copy.head()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#checking for linear relationship b/w dependent variable and Independent variable\n",
        "\n",
        "sns.lmplot(x='Mean_OHL', y='Close', fit_reg=True, data=dataset_copy)"
      ],
      "metadata": {
        "id": "xqce7zCihy7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use lags as additional features\n",
        "for i in range(1, 13):\n",
        "    dataset_copy[\"lag_{}\".format(i)] = dataset_copy.Mean_OHL.shift(i)"
      ],
      "metadata": {
        "id": "1HeFJAw8h0cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "\n",
        "#def calc_vif2(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    #vif = pd.DataFrame()\n",
        "    #vif[\"variables\"] = X.columns\n",
        "    #vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    #return(vif)\n",
        "\n",
        "#calc_vif2(dataset_copy[[i for i in dataset_copy.describe().columns if i not in ['Close']]])\n"
      ],
      "metadata": {
        "id": "VKXQOmQbh7cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I am using only Mean_OHL column and lag values, which will take the previous month OHL value. It will be useful to cover underlying patterns in this kind of problem."
      ],
      "metadata": {
        "id": "LR-N1NUkh_1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy.head()"
      ],
      "metadata": {
        "id": "ggea1QMLh_Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "#independent_variables1=['Open','Price_Range']\n",
        "#independent_variables1\n",
        "y_depend = dataset_copy.dropna().Close.values\n",
        "x_independ = dataset_copy.dropna().drop(['Close','Open','High','Low'], axis=1)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dependent_variable"
      ],
      "metadata": {
        "id": "1qYsNsz_iU62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open,High, close. I think they must be included because, on opening price is high and when low, there is a significant change in closing price."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,independent variables and dependent variables have skeweness. I have used log transformation as the skewness is small."
      ],
      "metadata": {
        "id": "HFZFaVPCio2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "\n",
        "x_independ.head()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Transform Your data\n",
        "\n",
        "x_independ['Mean_OHL'] = np.log10(x_independ['Mean_OHL'])\n",
        "\n",
        "# Create the dependent variable data\n",
        "Y = np.log10(y_depend)\n",
        "\n",
        "x_independ.values"
      ],
      "metadata": {
        "id": "1W26P49diwze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#without transformation\n",
        "#X_not =\n",
        "\n",
        "# Create the dependent variable data\n",
        "#Y_not = np.log10(y_depend)"
      ],
      "metadata": {
        "id": "Zo-QopZii1J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "\n",
        "#after train_test_split\n",
        "scaler = StandardScaler()\n",
        "x_scaled = scaler.fit_transform(x_independ.drop('Month', axis=1).values)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without transformation\n",
        "#scaler = StandardScaler()\n",
        "#X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "Mm5kjo-RjM_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to use standard scaler."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I have only few features, dimensionality reduction is not necessary\n",
        "\n",
        "Price Range: Instead of using Open, High, and Low separately, you can calculate the price range as the difference between the High and Low prices. This variable represents the volatility or fluctuation in the stock price for each period and can provide additional information to the model.\n",
        "\n",
        "Average Price: Another approach is to calculate the average price as the average of the Open, High, and Low prices. This variable represents the overall price level for each period and can be useful in capturing the general trend or level of the stock price."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "#splitting the data into a train and a test set. we do this using train test split.\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, Y, test_size = 0.2, random_state = 1)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without transformation\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size = 0.2, random_state = 0)\n",
        "#print(X_train.shape)\n",
        "#print(X_test.shape)\n"
      ],
      "metadata": {
        "id": "96MDN4s5jkbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used 80 percent of the data for training the model and 20 % for testing the model."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I do not think the data is imbalanced."
      ],
      "metadata": {
        "id": "ym9lYRoBjwyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are implementing Basic Linear regression model.\n",
        "\n",
        "The linear regression model assumes that the relationship between the dependent variable (Close) and the independent variables is linear, which means that the change in the dependent variable is proportional to the change in the independent variables.\n",
        "\n",
        "During the training process, the linear regression model tries to find the values of the coefficients that minimize the sum of the squared differences between the predicted values and the actual values of the dependent variable."
      ],
      "metadata": {
        "id": "F7ddonnQj6jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "reg_with_transformation = LinearRegression().fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_with_transformation.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "nFVZmdjNljtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Predicting our test data.\n",
        "y_train_pred_with_transformation= reg_with_transformation.predict(x_train)\n",
        "y_test_pred_with_transformation = reg_with_transformation.predict(x_test)\n",
        "#y_test_pred_without_transformation = reg_without_transformation.predict(X_test)"
      ],
      "metadata": {
        "id": "oNXMqbXNloHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "comparision_trans = pd.DataFrame(zip(10**(y_test), 10**(y_test_pred_with_transformation)), columns = ['actual', 'pred'])\n",
        "comparision_trans.head()"
      ],
      "metadata": {
        "id": "TZMCvLiAlsH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_MAE = mean_absolute_error(10**(y_train),(10**y_train_pred_with_transformation))\n",
        "print(f\"Mean Absolute Error : {train_MAE}\")\n",
        "\n",
        "\n",
        "train_MSE  = mean_squared_error(10**(y_train), 10**(y_train_pred_with_transformation))\n",
        "print(\"MSE :\" , train_MSE)\n",
        "\n",
        "train_RMSE = np.sqrt(train_MSE)\n",
        "print(\"RMSE :\" ,train_RMSE)\n",
        "\n",
        "train_r2 = r2_score(10**(y_train), 10**(y_train_pred_with_transformation))\n",
        "print(\"R2 :\" ,train_r2)\n",
        "\n",
        "train_adjusted_r2=1-(1-r2_score(10**(y_train), 10**(y_train_pred_with_transformation)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "MAE = mean_absolute_error(10**(y_test),(10**y_test_pred_with_transformation))\n",
        "print(f\"Mean Absolute Error : {MAE}\")\n",
        "\n",
        "MSE  = mean_squared_error(10**(y_test), 10**(y_test_pred_with_transformation))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test), 10**(y_test_pred_with_transformation))\n",
        "print(\"R2 :\" ,r2)\n",
        "\n",
        "adjusted_r2=1-(1-r2_score(10**(y_test), 10**(y_test_pred_with_transformation)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2)"
      ],
      "metadata": {
        "id": "i-63wtTUlxaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "metrics = ['MAE','MSE', 'RMSE', 'R2', 'Adjusted R2']\n",
        "scores = [MAE,MSE,RMSE,r2,adjusted_r2]\n",
        "\n",
        "# Plot the evaluation metric score chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel('Evaluation Metric')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Metric Score Chart for Linear Regression with Transformation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#visualizing actual and predicted data\n",
        "\n",
        "\n",
        "fig, (ax1) = plt.subplots(1, 1, figsize=(5, 3))\n",
        "\n",
        "# Plot with transformation\n",
        "ax1.plot(10 ** (y_test_pred_with_transformation))\n",
        "ax1.plot(np.array(10 ** (y_test)))\n",
        "ax1.legend([\"Predicted\", \"Actual\"])\n",
        "ax1.set_title(\"Predicted vs Actual (with Transformation)\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N5DNHJEAl_kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig=plt.figure(figsize=(8,8))\n",
        "\n",
        "sns.distplot((10**(y_test)- 10**(y_test_pred_with_transformation)),bins=20)\n",
        "\n",
        "#Plot Label\n",
        "fig.suptitle('Residual Analysis', fontsize = 20)"
      ],
      "metadata": {
        "id": "aYguJdq-pEeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "residuals = 10**(y_test)-10**(y_test_pred_with_transformation)\n",
        "\n",
        "# Plot the residuals against the predicted values\n",
        "plt.scatter( 10**(y_test_pred_with_transformation),residuals)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs. Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3A-VJ4I1pIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "linear_regessor_list = {'Train Mean Absolute Error':train_MAE,'Train Mean squared Error' : train_MSE,'Train Root Mean squared Error' : train_RMSE,'Train R2 score' : train_r2,'Train Adjusted R2 score' : train_adjusted_r2,'Mean Absolute Error':MAE,'Mean squared Error' : MSE,'Root Mean squared Error' : RMSE,'R2 score' : r2,'Adjusted R2 score' : adjusted_r2 }\n",
        "metrics = pd.DataFrame.from_dict(linear_regessor_list, orient='index').reset_index()\n",
        "metrics = metrics.rename(columns={'index':'Metric',0:'reg_with_transformation'})\n",
        "metrics"
      ],
      "metadata": {
        "id": "CyEDaknBpMiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Root Mean Squared Error (RMSE) is slightly higher on the test set (15.728903) compared to the training set (12.874182), indicating a slightly larger average magnitude of error in predicting the 'Close' values on the test set.\n",
        "\n",
        "The R2 Score is slightly lower on the test set (0.962720) compared to the training set (0.984231), suggesting that the model explains a slightly lower proportion of the variance in the 'Close' variable on the test set.\n",
        "\n",
        "The Adjusted R2 Score is also lower on the test set (0.939641) compared to the training set (0.982578), accounting for the complexity of the model and the number of independent variables.\n",
        "\n",
        "Overall, the model performs well on both the training and test sets, but there is a slightly higher level of error and slightly lower explanatory power on the test set, which is expected as the test set represents unseen data."
      ],
      "metadata": {
        "id": "W5Dna44UpQRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameter = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'copy_X': [True, False],\n",
        "\n",
        "    'positive': [True, False]\n",
        "}\n",
        "\n",
        "# Create the grid search object\n",
        "Lr_gs=GridSearchCV(reg_with_transformation,param_grid=parameter,cv=5,scoring='r2')\n",
        "\n",
        "# Fit the Algorithm\n",
        "Lr_gs.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_test_gs=Lr_gs.predict(x_test)\n",
        "y_pred_train_gs=Lr_gs.predict(x_train)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "train_MAE_gs = mean_absolute_error(10**(y_train),(10**y_pred_train_gs))\n",
        "print(f\"Mean Absolute Error : {train_MAE_gs}\")\n",
        "\n",
        "\n",
        "train_MSE_gs  = mean_squared_error(10**(y_train), 10**(y_pred_train_gs))\n",
        "print(\"MSE :\" , train_MSE_gs)\n",
        "\n",
        "train_RMSE_gs = np.sqrt(train_MSE_gs)\n",
        "print(\"RMSE :\" ,train_RMSE_gs)\n",
        "\n",
        "train_r2_gs = r2_score(10**(y_train), 10**(y_pred_train_gs))\n",
        "print(\"R2 :\" ,train_r2_gs)\n",
        "\n",
        "train_adjusted_r2_gs=1-(1-r2_score(10**(y_train), 10**(y_pred_train_gs)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_gs)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_gs = mean_absolute_error(10**(y_test),(10**y_pred_test_gs))\n",
        "print(f\"Mean Absolute Error : {MAE_gs}\")\n",
        "\n",
        "MSE_gs  = mean_squared_error(10**(y_test), 10**(y_pred_test_gs))\n",
        "print(\"MSE :\" , MSE_gs)\n",
        "\n",
        "RMSE_gs = np.sqrt(MSE_gs)\n",
        "print(\"RMSE :\" ,RMSE_gs)\n",
        "\n",
        "r2_gs = r2_score(10**(y_test), 10**(y_pred_test_gs))\n",
        "print(\"R2 :\" ,r2_gs)\n",
        "\n",
        "adjusted_r2_gs=1-(1-r2_score(10**(y_test), 10**(y_pred_test_gs)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_gs)"
      ],
      "metadata": {
        "id": "SNOHbS85meeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE and RMSE values for the test set are lower than those for the train set, indicating better performance on the test data.\n",
        "\n",
        "The R2 score for the test set is slightly higher than that for the train set, suggesting that the model generalizes well to unseen data.\n",
        "\n",
        "However, the adjusted R2 score for the test set is lower than that for the train set, indicating that the model may be overfitting to the training data.\n",
        "\n",
        "Overall, the model shows good performance on both the train and test sets, with low errors and high R2 scores. However, it is important to monitor the adjusted R2 score and consider potential overfitting when interpreting the results. To overcome that, we can apply regularization techniques."
      ],
      "metadata": {
        "id": "wFFlAB7enx0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "\n",
        "# Plot with transformation\n",
        "plt.plot(10 ** (y_pred_test_gs))\n",
        "plt.plot(np.array(10 ** (y_test)))\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "plt.title(\"Predicted vs Actual (with Transformation)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Fkpe8j38nqn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig=plt.figure(figsize=(4,4))\n",
        "\n",
        "sns.distplot((10**(y_test)- 10**(y_pred_test_gs)),bins=20)\n",
        "\n",
        "#Plot Label\n",
        "fig.suptitle('Residual Analysis', fontsize = 20)"
      ],
      "metadata": {
        "id": "fSbntOaKn5-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Homoscadasticity\n",
        "residuals = 10**(y_test)-10**(y_pred_test_gs)\n",
        "\n",
        "# Plot the residuals against the predicted values\n",
        "plt.scatter( 10**(y_pred_test_gs),residuals)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs. Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FCz6IOlbn-Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.mean(residuals)"
      ],
      "metadata": {
        "id": "DohtDF5roCEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the assumptions of Linear Regression is being taken care. Mean of Residuals is nearly zero, and there is no hetroscadasticity."
      ],
      "metadata": {
        "id": "1rbvpR0AoFcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics['Lr_gs'] = [train_MAE_gs, train_MSE_gs, train_RMSE_gs, train_r2_gs, train_adjusted_r2_gs,MAE_gs,MSE_gs,RMSE_gs,r2_gs,adjusted_r2_gs]\n"
      ],
      "metadata": {
        "id": "jUItJjJOprga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "I used GridSearchCV hyperparameter optimization technique which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.\n",
        "\n",
        "In GridSearchCV,cross-validation is also performed which is used while training the model."
      ],
      "metadata": {
        "id": "CcWN86YTomHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "XuFf-X_Lox60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "dbV8RXQsnflj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VUqoHFxznfIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to implement RandomForest model.\n",
        "\n",
        "Random forest is an ensemble learning algorithm that constructs a multitude of decision trees at training time and outputs the mean prediction of the individual trees as the final prediction.\n",
        "\n",
        "The RandomForestRegressor class allows you to train a regression model using the random forest algorithm, and then use it to make predictions on new data.\n",
        "\n",
        "During the training process, the algorithm randomly selects a subset of features and a subset of observations to construct each decision tree.\n",
        "\n",
        "This helps to reduce overfitting and improve the generalization performance of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "2eZxWbsxqH4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_rf =rf.predict(x_train)\n",
        "y_pred_test_rf =rf.predict(x_test)"
      ],
      "metadata": {
        "id": "-mZisPJ2qLZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Metric Score for train set\n",
        "train_MAE_rf = mean_absolute_error(10**(y_train),(10**y_pred_train_rf))\n",
        "print(f\"Mean Absolute Error : {train_MAE_rf}\")\n",
        "\n",
        "\n",
        "train_MSE_rf  = mean_squared_error(10**(y_train), 10**(y_pred_train_rf))\n",
        "print(\"MSE :\" , train_MSE_rf)\n",
        "\n",
        "train_RMSE_rf = np.sqrt(train_MSE_rf)\n",
        "print(\"RMSE :\" ,train_RMSE_rf)\n",
        "\n",
        "train_r2_rf = r2_score(10**(y_train), 10**(y_pred_train_rf))\n",
        "print(\"R2 :\" ,train_r2_rf)\n",
        "\n",
        "train_adjusted_r2_rf=1-(1-r2_score(10**(y_train), 10**(y_pred_train_rf)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_rf)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_rf = mean_absolute_error(10**(y_test),(10**y_pred_test_rf))\n",
        "print(f\"Mean Absolute Error : {MAE_rf}\")\n",
        "\n",
        "MSE_rf  = mean_squared_error(10**(y_test), 10**(y_pred_test_rf))\n",
        "print(\"MSE :\" , MSE_rf)\n",
        "\n",
        "RMSE_rf = np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\" ,RMSE_rf)\n",
        "\n",
        "r2_rf = r2_score(10**(y_test), 10**(y_pred_test_rf))\n",
        "print(\"R2 :\" ,r2_rf)\n",
        "\n",
        "adjusted_r2_rf=1-(1-r2_score(10**(y_test), 10**(y_pred_test_rf)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_rf)"
      ],
      "metadata": {
        "id": "wUnQy9JzqQ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be clearly seen that model is not performed well on unseen data indicating overfitting. Lets overcome that by tuning hyperparameter and using cross validation"
      ],
      "metadata": {
        "id": "nwl6bJk0qXDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.plot(10**((y_pred_test_rf)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kaPZDp22qgUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50,80,100,200,300],\n",
        "    'max_depth': [1,2,6,7,8,9,10,20,30,40],\n",
        "    'min_samples_split':[10,20,30,40,50,100,150,200],\n",
        "    'min_samples_leaf': [1,2,8,10,20,40,50]\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(rf, param_grid_rf,verbose=2, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best estimator\n",
        "best_model_rf_rs = random_search.best_estimator_"
      ],
      "metadata": {
        "id": "8xpHF5Tnqwoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_model_rf_rs.feature_importances_"
      ],
      "metadata": {
        "id": "YnimvinPq1l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model_rf_rs)\n"
      ],
      "metadata": {
        "id": "yE7KDylTq66f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predict the model\n",
        "y_pred_train_rf_rs= random_search.predict(x_train)\n",
        "y_pred_test_rf_rs= random_search.predict(x_test)"
      ],
      "metadata": {
        "id": "CYShOPLbq-YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "random_search.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "SPTgUFJzrBbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "train_MAE_rf_rs = mean_absolute_error(10**(y_train),(10**y_pred_train_rf_rs))\n",
        "print(f\"Mean Absolute Error : {train_MAE_rf_rs}\")\n",
        "\n",
        "\n",
        "train_MSE_rf_rs  = mean_squared_error(10**(y_train), 10**(y_pred_train_rf_rs))\n",
        "print(\"MSE :\" , train_MSE_rf_rs)\n",
        "\n",
        "train_RMSE_rf_rs = np.sqrt(train_MSE_rf_rs)\n",
        "print(\"RMSE :\" ,train_RMSE_rf_rs)\n",
        "\n",
        "train_r2_rf_rs = r2_score(10**(y_train), 10**(y_pred_train_rf_rs))\n",
        "print(\"R2 :\" ,train_r2_rf_rs)\n",
        "\n",
        "train_adjusted_r2_rf_rs=1-(1-r2_score(10**(y_train), 10**(y_pred_train_rf_rs)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_rf_rs)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_rf_rs = mean_absolute_error(10**(y_test),(10**y_pred_test_rf_rs))\n",
        "print(f\"Mean Absolute Error : {MAE_rf_rs}\")\n",
        "\n",
        "MSE_rf_rs  = mean_squared_error(10**(y_test), 10**(y_pred_test_rf_rs))\n",
        "print(\"MSE :\" , MSE_rf_rs)\n",
        "\n",
        "RMSE_rf_rs = np.sqrt(MSE_rf_rs)\n",
        "print(\"RMSE :\" ,RMSE_rf_rs)\n",
        "\n",
        "r2_rf_rs = r2_score(10**(y_test), 10**(y_pred_test_rf_rs))\n",
        "print(\"R2 :\" ,r2_rf_rs)\n",
        "\n",
        "adjusted_r2_rf_rs=1-(1-r2_score(10**(y_test), 10**(y_pred_test_rf_rs)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_rf_rs)\n",
        "\n"
      ],
      "metadata": {
        "id": "yIw3lzDlrLJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has improved than the simple random forest model. it has low adjusted r2 value in test data saying that some additional features are not contributing to the output of y variable."
      ],
      "metadata": {
        "id": "HH2SgxhKrRH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.plot(10**((y_pred_test_rf_rs)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zi_wP1bVrUDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics['random_search'] = [train_MAE_rf_rs, train_MSE_rf_rs, train_RMSE_rf_rs, train_r2_rf_rs, train_adjusted_r2_rf_rs,MAE_rf_rs,MSE_rf_rs,RMSE_rf_rs,r2_rf_rs,adjusted_r2_rf_rs]\n"
      ],
      "metadata": {
        "id": "zjcgtFBhrYKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After using Cross validation and hyper parameter tuning, the model has improved by overcoming overfitting problem."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R2 score:\n",
        "\n",
        "A high R2 score suggests that the model is able to explain a large portion of the variance in the data. In a business context, a high R2 score can indicate that the model is able to make accurate predictions, which could have a positive impact on decision-making.\n",
        "\n",
        "Adjusted R2 score:\n",
        "\n",
        "In a business context, a high adjusted R2 score can indicate that the model is able to make accurate predictions with a reasonable level of complexity, which could be more practical for deployment in a business setting.\n",
        "\n",
        "Mean absolute error (MAE):\n",
        "\n",
        "The MAE is a measure of the average absolute error of the model's predictions.\n",
        "\n",
        "In a business context, a low MAE can indicate that the model is making relatively small errors, which could be important if the model is being used to make important decisions.\n",
        "\n",
        "Root mean squared error (RMSE):\n",
        "\n",
        "The RMSE is a measure of the average squared error of the model's predictions.\n",
        "\n",
        "In a business context, a low RMSE can indicate that the model is making relatively small errors, which could be important if the model is being used to make important decisions."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 3 Implementation\n",
        "\n",
        "xgboost = XGBRegressor(objective= 'reg:squarederror')\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgboost.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_xg =xgboost.predict(x_train)\n",
        "y_pred_test_xg =xgboost.predict(x_test)"
      ],
      "metadata": {
        "id": "fd8QtJ4VwtuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_actual = np.power(10, y_train)\n",
        "y_train_pred   = np.power(10, y_pred_train_xg)\n",
        "\n",
        "y_test_actual  = np.power(10, y_test)\n",
        "y_test_pred    = np.power(10, y_pred_test_xg)\n"
      ],
      "metadata": {
        "id": "EuaU32MgxSw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To view the first few elements of y_train (a NumPy array), use slicing:\n",
        "print(y_train[:5])"
      ],
      "metadata": {
        "id": "YaBRB8Z-xdYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Train Metrics\n",
        "train_MAE_xg = mean_absolute_error(y_train_actual, y_train_pred)\n",
        "train_MSE_xg = mean_squared_error(y_train_actual, y_train_pred)\n",
        "train_RMSE_xg = np.sqrt(train_MSE_xg)\n",
        "train_r2_xg = r2_score(y_train_actual, y_train_pred)\n",
        "\n",
        "print(\"Train MAE:\", train_MAE_xg)\n",
        "print(\"Train RMSE:\", train_RMSE_xg)\n",
        "print(\"Train R2:\", train_r2_xg)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Test Metrics\n",
        "MAE_xg = mean_absolute_error(y_test_actual, y_test_pred)\n",
        "MSE_xg = mean_squared_error(y_test_actual, y_test_pred)\n",
        "RMSE_xg = np.sqrt(MSE_xg)\n",
        "r2_xg = r2_score(y_test_actual, y_test_pred)\n",
        "\n",
        "print(\"Test MAE:\", MAE_xg)\n",
        "print(\"Test RMSE:\", RMSE_xg)\n",
        "print(\"Test R2:\", r2_xg)\n"
      ],
      "metadata": {
        "id": "y12CK1OxxmlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "train_MAE_xg = mean_absolute_error(10**(y_train),(10**y_pred_train_xg))\n",
        "print(f\"Mean Absolute Error : {train_MAE_xg}\")\n",
        "\n",
        "\n",
        "train_MSE_xg  = mean_squared_error(10**(y_train), 10**(y_pred_train_xg))\n",
        "print(\"MSE :\" , train_MSE_xg)\n",
        "\n",
        "train_RMSE_xg = np.sqrt(train_MSE_xg)\n",
        "print(\"RMSE :\" ,train_RMSE_xg)\n",
        "\n",
        "train_r2_xg = r2_score(10**(y_train), 10**(y_pred_train_xg))\n",
        "print(\"R2 :\" ,train_r2_xg)\n",
        "\n",
        "train_adjusted_r2_xg=1-(1-r2_score(10**(y_train), 10**(y_pred_train_xg)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_xg)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_xg = mean_absolute_error(10**(y_test),(10**y_pred_test_xg))\n",
        "print(f\"Mean Absolute Error : {MAE_xg}\")\n",
        "\n",
        "MSE_xg  = mean_squared_error(10**(y_test), 10**(y_pred_test_xg))\n",
        "print(\"MSE :\" , MSE_xg)\n",
        "\n",
        "RMSE_xg = np.sqrt(MSE_xg)\n",
        "print(\"RMSE :\" ,RMSE_xg)\n",
        "\n",
        "r2_xg = r2_score(10**(y_test), 10**(y_pred_test_xg))\n",
        "print(\"R2 :\" ,r2_xg)\n",
        "\n",
        "adjusted_r2_xg=1-(1-r2_score(10**(y_test), 10**(y_pred_test_xg)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_xg)"
      ],
      "metadata": {
        "id": "cWDXrWIKwu_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost model.\n",
        "It is a popular machine learning algorithm that uses an ensemble of decision trees to make predictions.\n",
        "\n",
        "The XGBRegressor class allows us to train a regression model using the XGBoost algorithm which is then used to make predictions on new data.\n",
        "\n",
        "The model is trained by fitting a sequence of decision trees to the training data, with each new tree trying to correct the errors of the previous trees.\n",
        "\n",
        "The final model is a weighted sum of these individual trees."
      ],
      "metadata": {
        "id": "3MGIFyLjr48V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 3 Implementation\n",
        "\n",
        "xgboost = XGBRegressor(objective= 'reg:squarederror')\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgboost.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_xg =xgboost.predict(x_train)\n",
        "y_pred_test_xg =xgboost.predict(x_test)\n"
      ],
      "metadata": {
        "id": "7gMs74bW3vv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Metric Score for train set\n",
        "train_MAE_xg = mean_absolute_error(10**(y_train),(10**y_pred_train_xg))\n",
        "print(f\"Mean Absolute Error : {train_MAE_xg}\")\n",
        "\n",
        "\n",
        "train_MSE_xg  = mean_squared_error(10**(y_train), 10**(y_pred_train_xg))\n",
        "print(\"MSE :\" , train_MSE_xg)\n",
        "\n",
        "train_RMSE_xg = np.sqrt(train_MSE_xg)\n",
        "print(\"RMSE :\" ,train_RMSE_xg)\n",
        "\n",
        "train_r2_xg = r2_score(10**(y_train), 10**(y_pred_train_xg))\n",
        "print(\"R2 :\" ,train_r2_xg)\n",
        "\n",
        "train_adjusted_r2_xg=1-(1-r2_score(10**(y_train), 10**(y_pred_train_xg)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_xg)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_xg = mean_absolute_error(10**(y_test),(10**y_pred_test_xg))\n",
        "print(f\"Mean Absolute Error : {MAE_xg}\")\n",
        "\n",
        "MSE_xg  = mean_squared_error(10**(y_test), 10**(y_pred_test_xg))\n",
        "print(\"MSE :\" , MSE_xg)\n",
        "\n",
        "RMSE_xg = np.sqrt(MSE_xg)\n",
        "print(\"RMSE :\" ,RMSE_xg)\n",
        "\n",
        "r2_xg = r2_score(10**(y_test), 10**(y_pred_test_xg))\n",
        "print(\"R2 :\" ,r2_xg)\n",
        "\n",
        "adjusted_r2_xg=1-(1-r2_score(10**(y_test), 10**(y_pred_test_xg)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_xg)"
      ],
      "metadata": {
        "id": "tPN6qZ8U31Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "mbAIe3h-4C0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "#Converting into readable format\n",
        "EM=['MAE','MSE','RMSE','r2','adjusted_r2']\n",
        "train_xg=[train_MAE_xg,train_MSE_xg,train_RMSE_xg,train_r2_xg,train_adjusted_r2_xg]\n",
        "test_xg=[MAE_xg,MSE_xg,RMSE_xg,r2_xg,adjusted_r2_xg]\n",
        "\n",
        "#final dataframe of parameters\n",
        "data_xg=pd.DataFrame({'Evalution Parameters': EM, 'Train':train_xg, 'Test':test_xg}).set_index('Evalution Parameters')\n",
        "data_xg\n"
      ],
      "metadata": {
        "id": "FsAkqg8V3-xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is completely learning the data including noise, that is why it is not performing well on test data. overfitting the train dataset."
      ],
      "metadata": {
        "id": "-17_Oziz4K2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot((10**y_pred_test_xg))\n",
        "plt.plot(np.array((10**y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "\n",
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [50,100,200,300],\n",
        "    'max_depth': [3,4,5,6],\n",
        "    'learning_rate': [0.01,0.05,0.1],\n",
        "    'subsample': [0.7,0.8,0.9],\n",
        "    'colsample_bytree': [0.7,0.8,0.9]\n",
        "}\n",
        "\n",
        "xgb_random = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,       # Try only 30 combinations\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,       # Use all CPU\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train\n",
        "xgb_random.fit(x_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_xgb = xgb_random.best_estimator_\n",
        "\n",
        "print(\"Best Parameters:\", xgb_random.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_xgb.feature_importances_"
      ],
      "metadata": {
        "id": "gQLhUTSA7-Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_xgb)"
      ],
      "metadata": {
        "id": "2k22RNq28QSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_xgb.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "jvafLzEM8bp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([name for name in globals() if \"y_pred\" in name])\n"
      ],
      "metadata": {
        "id": "N67SblhV-bXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_actual = np.power(10, y_train)\n",
        "y_train_pred   = np.power(10, y_pred_train_gs)\n",
        "\n",
        "y_test_actual  = np.power(10, y_test)\n",
        "y_test_pred    = np.power(10, y_pred_test_gs)\n"
      ],
      "metadata": {
        "id": "753avYI0-vRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# -------- TRAIN --------\n",
        "\n",
        "train_MAE = mean_absolute_error(y_train_actual, y_train_pred)\n",
        "train_MSE = mean_squared_error(y_train_actual, y_train_pred)\n",
        "train_RMSE = np.sqrt(train_MSE)\n",
        "train_r2 = r2_score(y_train_actual, y_train_pred)\n",
        "\n",
        "train_adj_r2 = 1 - (1-train_r2)*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "\n",
        "\n",
        "print(\"----- Train Performance (XGBoost + GridSearch) -----\")\n",
        "print(\"MAE :\", train_MAE)\n",
        "print(\"MSE :\", train_MSE)\n",
        "print(\"RMSE:\", train_RMSE)\n",
        "print(\"R2  :\", train_r2)\n",
        "print(\"Adj R2:\", train_adj_r2)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# -------- TEST --------\n",
        "\n",
        "test_MAE = mean_absolute_error(y_test_actual, y_test_pred)\n",
        "test_MSE = mean_squared_error(y_test_actual, y_test_pred)\n",
        "test_RMSE = np.sqrt(test_MSE)\n",
        "test_r2 = r2_score(y_test_actual, y_test_pred)\n",
        "\n",
        "test_adj_r2 = 1 - (1-test_r2)*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "\n",
        "\n",
        "print(\"----- Test Performance (XGBoost + GridSearch) -----\")\n",
        "print(\"MAE :\", test_MAE)\n",
        "print(\"MSE :\", test_MSE)\n",
        "print(\"RMSE:\", test_RMSE)\n",
        "print(\"R2  :\", test_r2)\n",
        "print(\"Adj R2:\", test_adj_r2)\n"
      ],
      "metadata": {
        "id": "aXyK71sz-0wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,3))\n",
        "\n",
        "# Actual values\n",
        "plt.plot(np.power(10, y_test), label=\"Actual Closing Price\")\n",
        "\n",
        "# Predicted values\n",
        "plt.plot(np.power(10, y_pred_test_gs), label=\"Predicted Closing Price\")\n",
        "\n",
        "plt.title(\"Actual vs Predicted Closing Price (XGBoost)\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qtPRPAsC_F7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "columns = [\n",
        "    'Train_MAE','Train_MSE','Train_RMSE','Train_R2','Train_Adj_R2',\n",
        "    'Test_MAE','Test_MSE','Test_RMSE','Test_R2','Test_Adj_R2'\n",
        "]\n",
        "\n",
        "results_df = pd.DataFrame(metrics, index=columns)\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "T-ankQua_kZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV hyperparameter optimization technique which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "It uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters.\n",
        "\n",
        "In GridSearchCV,cross-validation is also performed which is used while training the model.\n",
        "\n",
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "-4Q6Px9o_0dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has improved a lot after tuning the hyperparameters"
      ],
      "metadata": {
        "id": "myD7f_i4_6GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered R2. adjusted R2 and RMSE as evaluation metrics.\n",
        "\n",
        "R2 score is a measure of how well the model fits the data.\n",
        "\n",
        "It ranges from 0 to 1, with a higher value indicating a better fit which means that the model is able to explain a large portion of the variance in the data which could have a positive impact on decision-making.\n",
        "\n",
        "The adjusted R-squared provides a penalized measure of model fit that takes into account both the explanatory power of the model and the complexity of the model. It is particularly useful when comparing models with different numbers of predictors. A higher adjusted R-squared indicates a better fit of the model, as it reflects the proportion of variance in the dependent variable that is explained by the independent variables, adjusted for the model complexity.\n",
        "\n",
        "RMSE is a measure of the average squared error of the model's predictions.\n",
        "\n",
        "It is calculated as the square root of the mean squared error (MSE).\n",
        "\n",
        "In a business context, a low RMSE can indicate that the model is making relatively small errors."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am considering XGBRegressor(xgb_gs) as my final model.\n",
        "\n",
        "This model has the highest R2 and adjusted r2 values on both the training and test sets, which indicates that it is doing a good job of explaining the variance in the target variable and also considering all the features. Ridge also performed well. but has low adjusted r2 in test data than in the train data. It indicates, that all the feautures are not necessary for ridge to predict the closing price, but I deliberately added lag values which captures previous data and pattern.\n",
        "\n",
        "XGboost capturing all features and still predicting better than ridge. It even has low RMSE among all the three models and also performed well on test data than on the train data."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: The XGBoost model does not have significant predictive power.\n",
        "\n",
        "Alternative Hypothesis: The XGBoost model has significant predictive power."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_xgb = xgb_random.best_estimator_\n"
      ],
      "metadata": {
        "id": "3JDgMEErAykG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_hypo = best_xgb.predict(x_test)\n"
      ],
      "metadata": {
        "id": "sB0Py55WA3H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Predictions from trained model\n",
        "y_pred_hypo = best_xgb.predict(x_test)\n",
        "\n",
        "# RSS and TSS\n",
        "RSS = np.sum((y_test - y_pred_hypo) ** 2)\n",
        "TSS = np.sum((y_test - np.mean(y_test)) ** 2)\n",
        "\n",
        "# Degrees of freedom\n",
        "n = len(y_test)\n",
        "k = x_test.shape[1]\n",
        "\n",
        "df_model = k\n",
        "df_residual = n - k - 1\n",
        "\n",
        "# F-statistic\n",
        "F = ((TSS - RSS) / df_model) / (RSS / df_residual)\n",
        "\n",
        "# p-value\n",
        "p_value = 1 - f.cdf(F, df_model, df_residual)\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "# Hypothesis Test\n",
        "if p_value < alpha:\n",
        "    print(\"Reject H0: Model has significant predictive power.\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: Model is not significant.\")\n",
        "\n",
        "print(\"F-statistic:\", F)\n",
        "print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "wP2fAocZA5GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "0ZiZrhyvBvxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ 1. Model Used: XGBoost Regression\n",
        "\n",
        "I used XGBoost Regressor, which is an ensemble machine learning algorithm based on boosting.\n",
        "\n",
        "üîπ What is XGBoost?\n",
        "\n",
        "XGBoost stands for Extreme Gradient Boosting.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Building many decision trees one by one\n",
        "\n",
        "Each new tree corrects the errors of previous trees\n",
        "\n",
        "All trees are combined to make the final prediction\n",
        "\n",
        "So, it is:\n",
        "\n",
        "‚úî An ensemble model\n",
        "‚úî Based on boosting technique\n",
        "‚úî Can be used for regression and classification\n",
        "\n",
        "In my case, I used it for regression, because the target variable is continuous.\n",
        "\n",
        "‚úÖ 2. Why XGBoost was Chosen\n",
        "\n",
        "I selected XGBoost because:\n",
        "\n",
        "‚úî Handles complex relationships\n",
        "‚úî Works well with large datasets\n",
        "‚úî Reduces overfitting using regularization\n",
        "‚úî Gives high accuracy\n",
        "‚úî Provides built-in feature importance\n",
        "\n",
        "‚úÖ 3. Model Explainability: Feature Importance\n",
        "\n",
        "To understand which features affect the prediction most, I used Feature Importance.\n",
        "\n",
        "Feature importance tells us:\n",
        "\n",
        "üëâ Which input variables contribute more to the model‚Äôs prediction.\n",
        "\n",
        "‚úÖ 4. Feature Importance Using XGBoost (Built-in Method)\n",
        "\n",
        "You can get feature importance using:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb.plot_importance(xgb_gs.best_estimator_)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "This plot shows:\n",
        "\n",
        "X-axis ‚Üí Importance score\n",
        "\n",
        "Y-axis ‚Üí Feature names\n",
        "\n",
        "Higher bar = More important feature ‚úÖ\n",
        "\n",
        "‚úÖ 5. Feature Importance Using SHAP (Advanced Explainability Tool)\n",
        "\n",
        "SHAP is a powerful explainability tool."
      ],
      "metadata": {
        "id": "IAkYKPWKBdJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "UjVql2h4BcsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = [f\"Feature_{i}\" for i in range(x_test.shape[1])]\n",
        "\n",
        "x_test_df = pd.DataFrame(x_test, columns=feature_names)"
      ],
      "metadata": {
        "id": "1Ih3mQ7I4DZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "\n",
        "# Create explainer\n",
        "explainer = shap.TreeExplainer(best_xgb)\n",
        "\n",
        "# Get SHAP values\n",
        "shap_values = explainer(x_test_df)\n",
        "\n",
        "# Initialize JS\n",
        "shap.initjs()\n",
        "\n",
        "# Force plot\n",
        "shap.force_plot(\n",
        "    explainer.expected_value,\n",
        "    shap_values.values[0],\n",
        "    x_test_df.iloc[0],\n",
        "    feature_names=x_test_df.columns\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "na2Ys1f00mTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SHAP values:\", shap_values.values.shape)\n",
        "print(\"Features:\", x_test_df.shape)\n",
        "print(\"Columns:\", len(x_test_df.columns))"
      ],
      "metadata": {
        "id": "BZikFT5BXAkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0]"
      ],
      "metadata": {
        "id": "DmwPetNq45GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The base value: The original paper explains that the base value is ‚Äúthe value that would be predicted if we did not know any features for the current output.‚Äù In other words, it is the mean prediction, or mean.\n",
        "\n",
        "Red/blue: Features that push the prediction higher (to the right) are shown in red, and those pushing the prediction lower are in blue.\n",
        "\n",
        "OHL: has a negative impact on the target variable.It has lower value than the average OHL value So it pushes the prediction to the right.\n",
        "\n",
        "lag_2 has a positive impact on the target variable. A higher than the average lag_2 value drives the prediction to the left.\n",
        "\n",
        "lag_4: is positively related to the target variable. A higher than the average lag_4 value pushes the prediction to the left."
      ],
      "metadata": {
        "id": "nU5PbVNh49Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "LjiRDbe746iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I plotted mean SHAP plot in which for each feature, we calculate the mean of the absolute SHAP values across all observations.\n",
        "\n",
        "There is one bar for each feature.\n",
        "\n",
        "Features that have large mean SHAP values will tend to have large positive/negative SHAP values. In other words, these are the features that have a significant impact on the model‚Äôs predictions.\n",
        "\n",
        "This plot can be used as a feature importance plot to highlight features that are important to a model‚Äôs predictions."
      ],
      "metadata": {
        "id": "efnUhzhJ5PBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.beeswarm(shap_values)"
      ],
      "metadata": {
        "id": "SLrGBDFO5TCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(best_xgb, open('/content/xgb_model.pkl', 'wb'))\n"
      ],
      "metadata": {
        "id": "a17_OzCaK-OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_xgb.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "rXdJii45PAi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(locals().keys())\n"
      ],
      "metadata": {
        "id": "7Na7aYWo6LGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "filename = \"/content/xgb_model.pkl\"\n",
        "\n",
        "pickle.dump(best_xgb, open(filename, \"wb\"))\n"
      ],
      "metadata": {
        "id": "1CcpWdE6PG1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "yesbank_ml_model = pickle.load(open(\"/content/xgb_model.pkl\", \"rb\"))\n"
      ],
      "metadata": {
        "id": "bCBRGrwnPMHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_unseen = yesbank_ml_model.predict(x_test)\n"
      ],
      "metadata": {
        "id": "ylNw1SexPRpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_test_actual = np.power(10, y_test)\n",
        "y_pred_actual = np.power(10, y_pred_unseen)\n",
        "\n",
        "print(\"R2:\", r2_score(y_test_actual, y_pred_actual))\n"
      ],
      "metadata": {
        "id": "lheDqA5gP0Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_actual = np.power(10, y_test)\n",
        "y_test_pred   = np.power(10, y_pred_unseen)\n",
        "\n",
        "plt.figure(figsize=(4,3))\n",
        "\n",
        "plt.plot(y_test_actual, label=\"Actual\")\n",
        "plt.plot(y_test_pred, label=\"Predicted\")\n",
        "\n",
        "plt.title(\"Sanity Check: Actual vs Predicted (Loaded Model)\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Closing Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V39hbDi98yM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main objective of this project is to develop a machine learning model that can accurately predict the monthly closing price of Yes Bank stock, while considering the impact of the fraud case that occurred in 2018.\n",
        "\n",
        "To achieve this, three models were developed and evaluated: Ridge Regression, Random Forest, and XGBoost Regressor. Among these, the XGBoost Regressor demonstrated the best performance, achieving an R¬≤ score of approximately 0.97 on both the training and testing datasets. This indicates strong predictive capability and good generalization. Additionally, the model effectively handles multicollinearity and incorporates all newly engineered features.\n",
        "\n",
        "Data visualization of the target variable clearly highlights the significant impact of the 2018 fraud case involving Rana Kapoor, during which the stock prices declined sharply. This confirms that major financial events strongly influence stock price movements.\n",
        "\n",
        "Further analysis revealed that most variables in the dataset were positively skewed. To address this issue and improve model performance, log transformation was applied to normalize the distributions.\n",
        "\n",
        "For feature engineering, the mean of the Open, High, and Low prices was calculated to form a new feature (OHL). In addition, multiple lag features were created to capture temporal trends and historical patterns. These lag features also help the model understand abnormal behavior during periods such as the fraud case.\n",
        "\n",
        "The most important features contributing to the prediction of the closing price were identified as:\n",
        "OHL, lag1, lag2, lag4, lag6, lag9, lag11, and lag12.\n",
        "\n",
        "One limitation of the dataset is that it contains only monthly stock prices. Using daily-level data could improve prediction accuracy by allowing the model to analyze short-term patterns such as weekly trends and weekend effects. Furthermore, including trading volume and external factors such as political events, economic policies, holidays, and unforeseen disasters could enhance model performance.\n",
        "\n",
        "Stock price prediction is influenced by many complex and dynamic factors. Therefore, incorporating additional features and applying advanced time-series models such as ARIMA and LSTM may lead to more accurate and robust predictions in the future.\n",
        "\n",
        "Overall, given the available dataset and engineered features, the developed XGBoost model performs well across all data points. With its high predictive accuracy and strong generalization capability, the model can be confidently deployed for future stock price forecasting tasks."
      ],
      "metadata": {
        "id": "hLKq5a_cuxr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}